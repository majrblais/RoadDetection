{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f4452d-6e50-4b2f-82b8-b825a0767784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n",
      "Processing with model: densenet121_1e-05_Unet_1024_noaug\n",
      "Processing image: image14.tif, size: (5000, 5000), crop size: 1024\n",
      "1/1 [==============================] - 24s 24s/step\n",
      "Saved original stitched image at ./mv_predictions\\densenet121_1e-05_Unet_1024_noaug_image14.tif_stitched.png\n",
      "Saved morphological operations output at ./mv_predictions\\densenet121_1e-05_Unet_1024_noaug_image14.tif_morph.png\n",
      "Saved contour filtering output at ./mv_predictions\\densenet121_1e-05_Unet_1024_noaug_image14.tif_contour.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\marcb\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py\", line 3441, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\marcb\\AppData\\Local\\Temp\\ipykernel_9228\\3566286634.py\", line 177, in <module>\n",
      "    cc_output = apply_connected_components_filtering(stitched_image)\n",
      "  File \"C:\\Users\\marcb\\AppData\\Local\\Temp\\ipykernel_9228\\3566286634.py\", line 125, in apply_connected_components_filtering\n",
      "    mask = labels_im == label\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\marcb\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\marcb\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\marcb\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\marcb\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"E:\\anaconda3\\envs\\segmentation\\lib\\inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"E:\\anaconda3\\envs\\segmentation\\lib\\inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"E:\\anaconda3\\envs\\segmentation\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"E:\\anaconda3\\envs\\segmentation\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"E:\\anaconda3\\envs\\segmentation\\lib\\ntpath.py\", line 664, in realpath\n",
      "    if _getfinalpathname(spath) == path:\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import numpy as np\n",
    "import segmentation_models as sm\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from multiprocessing import Pool\n",
    "# Define the models with their weights paths and corresponding crop sizes\n",
    "#    'densenet201_1e-05_Unet_1024_noaug': {'weights_path': './tosave/densenet201_1e-05_Unet_1024_noaug/Unet_densenet201_1e-05.h5','crop_size': 1024},\n",
    "\n",
    "\n",
    "'''\n",
    "    'densenet121_1e-05_Unet_1024_aug': {\n",
    "        'weights_path': './tosave/densenet121_1e-05_Unet_1024_aug/Unet_densenet121_best.h5',\n",
    "        'crop_size': 1024\n",
    "    },\n",
    "    \n",
    "\n",
    "'''\n",
    "\n",
    "# Define the models with their weights paths and corresponding crop sizes\n",
    "models_info = {\n",
    "    'densenet121_1e-05_Unet_1024_noaug': {\n",
    "        'weights_path': './tosave/densenet121_1e-05_Unet_1024_noaug/Unet_densenet121_best.h5',\n",
    "        'crop_size': 1024\n",
    "    },\n",
    "    'densenet121_1e-05_Unet_128_aug': {\n",
    "        'weights_path': './tosave/densenet121_1e-05_Unet_128_aug/Unet_densenet121_best.h5',\n",
    "        'crop_size': 128\n",
    "    },\n",
    "    'densenet121_1e-05_Unet_128_noaug': {\n",
    "        'weights_path': './tosave/densenet121_1e-05_Unet_128_noaug/Unet_densenet121_best.h5',\n",
    "        'crop_size': 128\n",
    "    },\n",
    "    'densenet121_1e-05_Unet_256_aug': {\n",
    "        'weights_path': './tosave/densenet121_1e-05_Unet_256_aug/Unet_densenet121_best.h5',\n",
    "        'crop_size': 256\n",
    "    },\n",
    "    'densenet121_1e-05_Unet_256_noaug': {\n",
    "        'weights_path': './tosave/densenet121_1e-05_Unet_256_noaug/Unet_densenet121_best.h5',\n",
    "        'crop_size': 256\n",
    "    },\n",
    "    'densenet121_1e-05_Unet_512_aug': {\n",
    "        'weights_path': './tosave/densenet121_1e-05_Unet_512_aug/Unet_densenet121_best.h5',\n",
    "        'crop_size': 512\n",
    "    },\n",
    "    'densenet121_1e-05_Unet_512_noaug': {\n",
    "        'weights_path': './tosave/densenet121_1e-05_Unet_512_noaug/Unet_densenet121_best.h5',\n",
    "        'crop_size': 512\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define input/output directories\n",
    "input_images_dir = './v3_val_small/images'\n",
    "output_dir = './mv_predictions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to crop images with overlap if not multiple of crop size\n",
    "def crop_and_resize_image(image, crop_size):\n",
    "    height, width = image.shape[:2]\n",
    "    crops = []\n",
    "    for y in range(0, height, crop_size):\n",
    "        for x in range(0, width, crop_size):\n",
    "            x_end = min(x + crop_size, width)\n",
    "            y_end = min(y + crop_size, height)\n",
    "            crop = image[y:y_end, x:x_end]\n",
    "            # Only resize if necessary\n",
    "            if crop.shape[0] != crop_size or crop.shape[1] != crop_size:\n",
    "                crop = cv2.resize(crop, (crop_size, crop_size))\n",
    "            crops.append((crop, x, y))\n",
    "    return crops\n",
    "\n",
    "# Function to stitch predictions together into one image\n",
    "def stitch_predictions(predictions, full_image_size):\n",
    "    full_height, full_width = full_image_size\n",
    "    stitched_image = np.zeros((full_height, full_width), dtype=np.uint8)\n",
    "    \n",
    "    for pred, x, y in predictions:\n",
    "        h, w = pred.shape\n",
    "\n",
    "        # Ensure pred fits into the image by adjusting the size if it exceeds boundaries\n",
    "        h = min(h, full_height - y)\n",
    "        w = min(w, full_width - x)\n",
    "\n",
    "        stitched_image[y:y+h, x:x+w] = np.maximum(stitched_image[y:y+h, x:x+w], pred[:h, :w])\n",
    "    \n",
    "    return stitched_image\n",
    "\n",
    "# Predict crops in a batch for efficiency\n",
    "def predict_crops_in_batch(crops, model, crop_size):\n",
    "    crop_batch = []\n",
    "    positions = []\n",
    "    for crop, x, y in crops:\n",
    "        resized_crop = cv2.resize(crop, (crop_size, crop_size))\n",
    "        crop_batch.append(resized_crop)\n",
    "        positions.append((x, y))\n",
    "    \n",
    "    crop_batch = np.array(crop_batch)\n",
    "    predictions = model.predict(crop_batch, batch_size=len(crop_batch))\n",
    "    \n",
    "    processed_predictions = []\n",
    "    for i, (x, y) in enumerate(positions):\n",
    "        pred = (predictions[i] > 0.5).astype(np.uint8) * 255\n",
    "        processed_predictions.append((pred[:, :, 0], x, y))\n",
    "        \n",
    "    return processed_predictions\n",
    "\n",
    "# Post-processing methods\n",
    "def apply_morphological_operations(image, kernel_size=5):\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    return cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "def apply_contour_filtering(image, area_threshold=500):\n",
    "    contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    filtered_image = image.copy()\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) < area_threshold:\n",
    "            cv2.drawContours(filtered_image, [contour], -1, 0, -1)  # Remove small contours\n",
    "    return filtered_image\n",
    "\n",
    "def apply_connected_components_filtering(image, size_threshold=500):\n",
    "    num_labels, labels_im = cv2.connectedComponents(image)\n",
    "    filtered_image = image.copy()\n",
    "    for label in range(1, num_labels):  # Start from 1 to skip the background\n",
    "        mask = labels_im == label\n",
    "        if np.sum(mask) < size_threshold:\n",
    "            filtered_image[mask] = 0\n",
    "    return filtered_image\n",
    "\n",
    "# Main processing loop\n",
    "for model_name, model_info in models_info.items():\n",
    "    print(f\"Processing with model: {model_name}\")\n",
    "    \n",
    "    # Load the model once\n",
    "    BACKBONE = model_name.split('_')[0]\n",
    "    preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "    model = sm.Unet(BACKBONE, classes=1, activation='sigmoid')\n",
    "    model.load_weights(model_info['weights_path'])\n",
    "\n",
    "    crop_size = model_info['crop_size']\n",
    "\n",
    "    # Loop through images in the input folder\n",
    "    for image_file in os.listdir(input_images_dir):\n",
    "        image_path = os.path.join(input_images_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "        original_size = image.shape[:2]\n",
    "        print(f\"Processing image: {image_file}, size: {original_size}, crop size: {crop_size}\")\n",
    "\n",
    "        # Preprocess and crop the image\n",
    "        preprocessed_image = preprocess_input(image)\n",
    "        crops = crop_and_resize_image(preprocessed_image, crop_size)\n",
    "\n",
    "        # Batch predict crops\n",
    "        predictions = predict_crops_in_batch(crops, model, crop_size)\n",
    "\n",
    "        # Stitch predictions together\n",
    "        stitched_image = stitch_predictions(predictions, original_size)\n",
    "\n",
    "        # Save original stitched image\n",
    "        original_output_path = os.path.join(output_dir, f\"{model_name}_{image_file}_stitched.png\")\n",
    "        cv2.imwrite(original_output_path, stitched_image)\n",
    "        print(f\"Saved original stitched image at {original_output_path}\")\n",
    "\n",
    "        # Apply morphological operations\n",
    "        morph_output = apply_morphological_operations(stitched_image)\n",
    "        morph_output_path = os.path.join(output_dir, f\"{model_name}_{image_file}_morph.png\")\n",
    "        cv2.imwrite(morph_output_path, morph_output)\n",
    "        print(f\"Saved morphological operations output at {morph_output_path}\")\n",
    "\n",
    "        # Apply contour filtering\n",
    "        contour_output = apply_contour_filtering(stitched_image)\n",
    "        contour_output_path = os.path.join(output_dir, f\"{model_name}_{image_file}_contour.png\")\n",
    "        cv2.imwrite(contour_output_path, contour_output)\n",
    "        print(f\"Saved contour filtering output at {contour_output_path}\")\n",
    "\n",
    "        # Apply connected components filtering\n",
    "        cc_output = apply_connected_components_filtering(stitched_image)\n",
    "        cc_output_path = os.path.join(output_dir, f\"{model_name}_{image_file}_cc.png\")\n",
    "        cv2.imwrite(cc_output_path, cc_output)\n",
    "        print(f\"Saved connected components filtering output at {cc_output_path}\")\n",
    "\n",
    "print(\"All predictions and post-processed outputs saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b42f5-b37c-433a-b3d8-335ab56a3bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
